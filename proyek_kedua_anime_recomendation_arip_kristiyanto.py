# -*- coding: utf-8 -*-
"""Proyek kedua Anime Recomendation- Arip Kristiyanto

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UE1tynh4YS_uFi-T_saGfYHRrGxLoT4Y

# Proyek kedua Anime Recomendation-Arip Kristiyanto


*   Nama : Arip Kristiyanto
*   Jenis Kelamin : Laki-Laki
*   Institusi : Universitas Pamulang

# **Import Librabry**
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow
from keras import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from zipfile import ZipFile
from tensorflow import keras
from keras import layers
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import ast
import warnings
from sklearn.model_selection import train_test_split

"""# Import Datasets"""

from google.colab import drive
drive.mount('/content/drive')

rating_df = pd.read_csv('/content//drive/MyDrive/dataset/rating.csv')

anime_df = pd.read_csv('/content//drive/MyDrive/dataset/anime.csv')

"""

Kedua sudah berhasil di-import kedalam notebook.

*   movies.csv menjadi dataframe dengan nama anime_df
*   ratings.csv menjadi dataframe dengan nama rating_df





  

"""

anime_df.head()

rating_df.head()

"""# Data Understanding

Dataset yang digunakan untuk pembuatan model system recommendation ini adalah dataset "Movie Magic: Data-Driven Recommendations" yang tersedia di situs[ kaggle](https://www.kaggle.com/code/dumanmesut/anime-recomendation-systems) yang berisi data-data mengenai movie beserta ratingd yang diberikan oleh para pembaca.

Terdapat 2 file didalamnya,  dataset animes.csv dan rating.csv. animes terdiri dari , baris 12294 data dan 7 kolom data. rating.csv terdiri dari 93045 baris data dan 3 kolom data.

Kedua dataset tersebut dapat digunakan untuk membuat system recommendation, baik Content-Based Filtering maupun Collaborative Filtering

Berikut ini adalah infomasi lainnya mengenai atribut-atribut yang terdapat pada dua dataset tersebut:

Atribut-atribut pada anime_df.csv:

* anime_id - identifikasi unik anime
* name - full name .
* genre - genre dari anime
* type - movie, TV, OVA, etc.
* episodes - berapa banyak episode dalam acara ini. (1 jika film)
* rating - peringkat rata-rata dari 10 untuk anime
* members - umlah anggota komunitas yang ada di grup anime

Atribut-atribut pada rating_df.csv:

* user_id - ID pengguna yang dibuat secara acak.
* anime_id - anime yang diberi peringkat oleh pengguna ini.
* rating - Peringkat dari 10 yang diberikan pengguna ini (-1 jika pengguna menontonnya tetapi tidak memberikan peringkat).

## Exploratory Data Analysis

Exploratory Data Analysis (EDA) adalah pendekatan analisis data yang bertujuan untuk memahami karakteristik utama dari kumpulan data. EDA melibatkan penggunaan teknik statistik dan visualisasi grafis untuk menemukan pola, hubungan, atau anomali untuk membentuk hipotesis. Proses ini sering kali tidak terstruktur dan dianggap sebagai langkah awal penting dalam analisis data yang membantu menentukan arah analisis lebih lanjut.

**anime_df**
"""

# Menampilan jumlah baris dan kolom yang ada pada dataset

anime_df.shape

"""
Berdasarkan output diatas, movies_df memiliki:

  
  * 12294 baris data
  * 7 kolom data

"""

# Menampilkan kolom-kolom yang ada pada dataset

anime_df.keys()

# Menampilkan tipe data dari setiap kolom yang ada

anime_df.info()

"""Berdasarkan output diatas 1 kolom bertipe data `float64` 4 kolom bertipe data `object` 2 kolom bertipe data `int64`"""

print('Banyak genre: ', len(anime_df.genre.unique()))
print('Tipe : ', anime_df.genre.unique())

"""Berdasarkan output diatas, animw_df terdapat 3265 genres

**ratings_df**
"""

# Menampilan jumlah baris dan kolom yang ada pada dataset

rating_df.shape

"""

Berdasarkan output diatas, movies_df memiliki:

  * 93045 baris data
  * 3 kolom data

"""

# Menampilkan kolom-kolom yang ada pada dataset

rating_df.keys()

"""Berdasarkan output diatas terdapat 4 kolom"""

rating_df.info()

"""Berdasarkan output diatas, rating_df memiliki 3 kolom dengan tipe data `int64`
   

"""

rating_df['rating'].describe()

"""

Fungsi diatas memberikan informasi statistika deskriptif untuk kolom rating, yaitu:

* count : Jumlah data dari sebuah kolom
* mean : Rata-rata dari sebuah kolom
* std : Standar deviasi dari sebuah kolom
* min : Nilai terendah pada sebuah kolom
* 25% : Nilai kuartil pertama (Q1) dari sebuah kolom
* 50% : Nilai kuartil kedua (Q2) atau median atau nilai tengah dari sebuah kolom
* 75% : Nilai kuartil ketiha (Q3) dari sebuah kolom
* max : Nilai tertinggi pada sebuah kolom


"""

anime_df.groupby('genre')['genre'].agg('count')

# Menampilakn total unique value di kolom user

print(rating_df['user_id'].nunique())

"""Berdasarkan output diatas, rating_df memiliki 958 user_id secara unique dari keseluruhan dataset. Hal ini berarti ada 958 user yang memberikan review terhadap film-film yang mereka telah tonton.

## Data Visualization



Visualisasi data adalah proses representasi grafis dari informasi dan data. Dengan menggunakan elemen visual seperti grafik, diagram, dan peta, visualisasi data menyediakan cara yang intuitif dan mudah diakses untuk melihat dan memahami tren, anomali, dan pola dalam data. Tujuan utama dari visualisasi data adalah untuk mengkomunikasikan informasi secara jelas dan efisien kepada pengguna, sehingga memudahkan pemahaman, analisis, dan pengambilan keputusan berdasarkan data tersebut

**anime_df**

**Univariate Analysis**


Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel (atau bidang data) pada satu waktu. Tujuannya adalah untuk menggambarkan data dan menemukan pola yang ada dalam distribusi variabel tersebut. Ini termasuk penggunaan statistik deskriptif, histogram, dan box plots untuk menganalisis distribusi dan memahami sifat dari variabel tersebut.
"""

# Anime Categories Distribution
from pandas.core.tools.datetimes import overload
ona = anime_df.loc[anime_df['type'] == 'ONA'].count()[0]
tv = anime_df.loc[anime_df['type'] == 'TV'].count()[0]
movie = anime_df.loc[anime_df['type'] == 'Movie'].count()[0]
music = anime_df.loc[anime_df['type'] == 'Music'].count()[0]
special = anime_df.loc[anime_df['type'] == 'Special'].count()[0]
ova = anime_df.loc[anime_df['type'] == 'OVA'].count()[0]

labels = ['ONA', 'TV', 'Movie', 'Music', 'Special', 'OVA']
colors = ['#81F4E1', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B']

plt.figure(figsize = (10,7))
plt.title('Anime Categories Distribution')
plt.pie([ona, tv, movie, music, special, ova],
        labels = labels,
        colors = colors,
        autopct = '%.2f %%'
        )

plt.show()

# Anime's Average Ratings Distribution
plt.hist(anime_df.rating, color='#B4E1FF', edgecolor='black')
plt.ylabel('Total')
plt.xlabel('Avg Rating')
plt.title("Anime's Average Ratings Distribution")
plt.show()

"""**rating_df**



Univariate Analysis

Univariate Analysis adalah jenis analisis data yang memeriksa satu variabel (atau bidang data) pada satu waktu. Tujuannya adalah untuk menggambarkan data dan menemukan pola yang ada dalam distribusi variabel tersebut. Ini termasuk penggunaan statistik deskriptif, histogram, dan box plots untuk menganalisis distribusi dan memahami sifat dari variabel tersebut.

"""

plt.figure(figsize=(10, 6))
review_counts = rating_df['rating'].value_counts()
review_counts_sorted = review_counts.sort_values(ascending=False)
sns.countplot(x='rating', data=rating_df, color='#30D5C8', order=review_counts_sorted.index)
sns.despine()
plt.title('Count Plot dari Rating anime')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

rating_percentages = rating_df['rating'].value_counts(normalize=True) * 100
plt.figure(figsize=(8, 6))
rating_percentages.plot.pie(autopct='%1.1f%%')
plt.title('Percentage of Each Rating Value')
plt.ylabel('')
plt.show()

"""**Multivariate Analysis**

Multivariate Analysis adalah prosedur statistik yang digunakan untuk memeriksa hubungan antara beberapa variabel secara bersamaan. Teknik ini mencakup berbagai metode seperti regresi berganda, analisis faktor, dan analisis kluster, yang membantu dalam memahami struktur dan pola yang kompleks dalam data dengan lebih dari satu variabel.
"""

# Top 10 Anime Community
anime_df.sort_values(by='members', ascending=False).head(10)

"""Menampilkan daftar anime dengan jumlah anggota community terbanyak. Misalnya, anime Death Note memiliki jumlah anggota community terbanyak, yaitu sebesar 1013917."""

# Top 10 Anime Community Plot
plt.figure(figsize = (20,15))
top10_anime = anime_df[['name', 'members']].sort_values(by = 'members',ascending = False).head(10)

colors = ['#87255B', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B', '#D3C4D1', '#81F4E1', '#C2AFF0', '#C57B57']


labels = top10_anime[['name']].values.flatten()
values = top10_anime[['members']].values.flatten()

plt.barh(labels, values, color = colors, edgecolor='black')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='x', alpha=0.7)
plt.xticks(fontsize = 15)
plt.yticks(fontsize = 15)
plt.title("Top 10 Anime Community", fontdict = {'fontsize' : 20})
plt.show()

plt.show()

"""## Missing value


Missing Values adalah data yang hilang atau tidak tercatat dalam dataset. Hal ini bisa terjadi karena berbagai alasan, seperti kesalahan entri data, kerusakan data, atau tidak tersedianya informasi saat pengumpulan data. Missing values dapat mempengaruhi kualitas model machine learning dan hasil analisis statistik. Oleh karena itu, penting untuk mengidentifikasi, menganalisis, dan mengatasi missing values dengan metode seperti imputasi, di mana nilai yang hilang diganti dengan estimasi, atau dengan menghapus baris atau kolom yang terdampak.

**anime_df**
"""

anime_df.isnull().sum()

"""Berdasarkan hasil diatas, terdapat  3 missing values yaitu `genre ` `type` `rating`

**rating_df**
"""

rating_df.isnull().sum()

"""Berdasarkan data diatas tidak ditemukan missing values

## Duplikat data



Data duplikat adalah baris data yang sama persis untuk setiap variabel yang ada. Dataset yang digunakan perlu diperiksa juga apakah dataset memiliki data yang sama atau data duplikat. Jika ada, maka data tersebut harus ditangani dengan menghapus data duplikat tersebut.

Alasan: Data duplikat perlu didektesi dan dihapus karena jika dibiarkan pada dataset dapat membuat model Anda memiliki bias, sehingga menyebabkan overfitting. Dengan kata lain, model memiliki performa akurasi yang baik pada data pelatihan, tetapi buruk pada data baru. Menghapus data duplikat dapat membantu memastikan bahwa model Anda dapat menemukan pola yang ada lebih baik lagi.
"""

# Cek baris duplikat dalam dataset
duplicates_anime = anime_df.duplicated()

# Hitung jumlah baris duplikat
duplicate_anime = duplicates_anime.sum()

# Cetak jumlah baris duplikat
print(f"Number of duplicate rows: {duplicate_anime}")

"""Berdasarkan hasil tersebut, tidak ditemukan adanya data duplikat, maka tidak ada juga proses penghapusannya.

**rating_df**
"""

# Cek baris duplikat dalam dataset
duplicates_rating =rating_df.duplicated()

# Hitung jumlah baris duplikat
duplicate_rating = duplicates_rating.sum()

# Cetak jumlah baris duplikat
print(f"Number of duplicate rows: {duplicate_rating}")

"""Berdasarkan hasil tersebut, ditemukan 1 data duplicate

## Outliers

Outliers adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Outliers adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Proses pembersihan outliers menggunakan metode IQR (Interquartile Range) melibatkan beberapa langkah:

  * Menghitung Kuartil: Tentukan kuartil pertama (Q1) dan kuartil ketiga (Q3) dari data. Kuartil ini membagi data menjadi empat bagian yang sama.

  * Menghitung IQR: Hitung IQR dengan mengurangi Q1 dari Q3:
    * IQR=Q3-Q1

  * Menentukan Batas Outliers:

    * Batas bawah untuk outliers:
      * Q1-1,5*IQR
    * Batas atas untuk outliers:
      * Q3+1,5*IQR

  * Identifikasi Outliers: Data yang berada di luar batas bawah dan atas ini dianggap sebagai outliers.

Pembersihan Outliers yang teridentifikasi kemudian dapat dibersihkan dari dataset, baik dengan menghapusnya atau melakukan transformasi tertentu.

Alasan:Outliers perlu dideteksi dan dihapus karena jika dibiarkan dapat merusak hasil analisis statistik pada kumpulan data sehingga menghasilkan performa model yang kurang baik. Selain itu, Mendeteksi dan menghapus outlier dapat membantu meningkatkan performa model Machine Learning menjadi lebih baik.
"""

rating_df['rating'].describe()

"""Sebelum memulai dengan proses interquartile. Perlu dilihat terlebih dahulu secara sekilas secara statistika deskriptif.

Hanya kolom review yang dicek karena hanya kolom tersebut yang tergolong sebagai kolom numeric dan perlu dilakukan pemeriksaan outliers-nya.

Berdasarkan output diatas, terlihbat bahwa nilai terkecil dari review adalah -1 dan terbesarnya adalah 10.

**Dataset rating anime memiliki rating terendah yang diberikan user pada suatu anime adalah -1 dan rating tertinggi adalah 10. Rating -1 menandakan bahwa user menonton anime, namun tidak memberikan rating.**

# Data Preparation

## Data Cleaning

### Removal Duplicates


Data duplikat adalah baris data yang sama persis untuk setiap variabel yang ada. Dataset yang digunakan perlu diperiksa juga apakah dataset memiliki data yang sama atau data duplikat. Jika ada, maka data tersebut harus ditangani dengan menghapus data duplikat tersebut.

Alasan: Data duplikat perlu didektesi dan dihapus karena jika dibiarkan pada dataset dapat membuat model Anda memiliki bias, sehingga menyebabkan overfitting. Dengan kata lain, model memiliki performa akurasi yang baik pada data pelatihan, tetapi buruk pada data baru. Menghapus data duplikat dapat membantu memastikan bahwa model Anda dapat menemukan pola yang ada lebih baik lagi.

**Setelah ditemukan 1 duplikat data maka akan kita hapus dalam rating_df**
"""

# hapus duplikat data
anime_df=anime_df.drop_duplicates()

"""berhasil dihapus duplikat data

### Handle Missing Value


Missing Value terjadi ketika variabel atau barus tertentu kekurangan titik data, sehingga menghasilkan informasi yang tidak lengkap. Nilai yang hilang dapat ditangani dengan berbagai cara seperti imputasi (mengisi nilai yang hilang dengan mean, median, modus, dll), atau penghapusan (menghilangkan baris atau kolom yang nilai hilang)

Alasan: Missing Value perlu ditangani karena jika dibiarkan dapat berpengaruh ke rendahnya akurasi model yang akan dibuat. Maka dari itu, penting untuk mengatasi missing value secara efisien untuk mendapatkan model Machine Learning yang baik juga.

**Berdasarkan hasil diatas, terdapat  3 missing values yaitu `genre ` `type` `rating`**

Berdasarkan output diatas, terdapat 2  missing value runtime , release_year
"""

anime_df.dropna(inplace =True)

anime_df.shape

"""Missing value Berhasil ditangani

### Outliers Detection and Removal
Outliers adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Outliers adalah titik data yang secara signifikan berbeda dari sebagian besar data dalam kumpulan data. Outliers dapat muncul karena variasi dalam pengukuran atau mungkin menunjukkan kesalahan eksperimental; dalam beberapa kasus, outliers bisa juga menunjukkan variabilitas yang sebenarnya dalam data. Penting untuk menganalisis outliers karena mereka dapat memiliki pengaruh besar pada hasil analisis statistik.

Proses pembersihan outliers menggunakan metode IQR (Interquartile Range) melibatkan beberapa langkah:

  * Menghitung Kuartil: Tentukan kuartil pertama (Q1) dan kuartil ketiga (Q3) dari data. Kuartil ini membagi data menjadi empat bagian yang sama.

  * Menghitung IQR: Hitung IQR dengan mengurangi Q1 dari Q3:
    * IQR=Q3-Q1

  * Menentukan Batas Outliers:

    * Batas bawah untuk outliers:
      * Q1-1,5*IQR
    * Batas atas untuk outliers:
      * Q3+1,5*IQR

  * Identifikasi Outliers: Data yang berada di luar batas bawah dan atas ini dianggap sebagai outliers.

Pembersihan Outliers yang teridentifikasi kemudian dapat dibersihkan dari dataset, baik dengan menghapusnya atau melakukan transformasi tertentu.

Alasan:Outliers perlu dideteksi dan dihapus karena jika dibiarkan dapat merusak hasil analisis statistik pada kumpulan data sehingga menghasilkan performa model yang kurang baik. Selain itu, Mendeteksi dan menghapus outlier dapat membantu meningkatkan performa model Machine Learning menjadi lebih baik.

**Berdasarkan output data understanding, terlihat bahwa nilai terkecil dari review adalah -1 dan terbesarnya adalah 10. Rating -1 menandakan bahwa user menonton anime, namun tidak memberikan rating. **
"""

rating_df = rating_df[~(rating_df.rating == -1)]

rating_df.describe().apply(lambda s: s.apply('{0:.2f}'.format))

"""Sebelum memulai dengan proses interquartile. Perlu dilihat terlebih dahulu secara sekilas secara statistika deskriptif.

Berdasarkan output diatas, terlihbat bahwa nilai terkecil dari score adalah 1 dan terbesarnya adalah 10.

### Menghapus symbol pada judul anime
"""

import re
def text_cleaning(text):
    text = re.sub(r'"', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'"', '', text)
    text = re.sub(r'A"s', '', text)
    text = re.sub(r'I"', 'I\'', text)
    text = re.sub(r'&', 'and', text)

    return text

anime_df['name'] = anime_df['name'].apply(text_cleaning)

"""## Encoding


Encoding adalah proses konversi informasi dari satu bentuk atau format ke bentuk lain, yang sering kali dilakukan untuk memastikan kompatibilitas dan pemrosesan yang tepat oleh berbagai sistem komputer. Proses ini sangat penting dalam dunia digital, di mana berbagai jenis data, seperti teks, gambar, dan suara, harus diubah menjadi format yang dapat dipahami oleh perangkat keras dan perangkat lunak.

Alasan: Tahap ini perlu dilakukan karena Encoding memungkinkan data dari berbagai sumber dan format untuk diubah menjadi format standar yang dapat dipahami dan memastikan bahwa informasi dapat diproses

"""

user_id = rating_df['user_id'].unique().tolist() # Mengubah userId menjadi list tanpa nilai yang sama
user_to_user = {x: i for i, x in enumerate(user_id)} # Melakukan encoding userId
user_encode_to_user = {i: x for i, x in enumerate(user_id)} # Melakukan proses encoding angka ke ke userId

print('list userId :  ', user_id)
print('encoded userId :  ', user_to_user)
print('encoded angka ke userId :  ', user_encode_to_user)

"""Berdasarkan output diatas, proses encoding untuk userId sudah berhasil dilakukan."""

anime_id = rating_df['anime_id'].unique().tolist() # Mengubah movieId menjadi list tanpa nilai yang sama
anime_to_anime = {x: i for i, x in enumerate(anime_id)} # Melakukan proses encoding movieId
anime_encode_to_anime = {i: x for i, x in enumerate(anime_id)} # Melakukan proses encoding angka ke movieId

print('list anime_id:  ', anime_id)
print('encoded anime_id:  ', anime_to_anime)
print('encoded angka ke anime_id:  ', anime_encode_to_anime)

"""Berdasarkan output diatas, proses encoding untuk `anime_id` sudah berhasil dilakukan."""

rating_df['user'] = rating_df['user_id'].map(user_to_user) # Mapping userId ke dataframe user
rating_df['anime'] = rating_df['anime_id'].map(anime_to_anime) # Mapping animeId ke dataframe resto

"""Hasil encoding tadi, di-mapping ke dalam dataframe review_df dengan menempati kolom baru untuk masing-masing hasil."""

rating_df.head(5)

"""Proses mapping berhasil dilakukan karena sudah terdapat dua kolom baru, yaitu user dan anime"""

num_users = len(user_to_user) # Mendapatkan jumlah user
num_anime = len(anime_to_anime) # Mendapatkan jumlah rating
min_rating = min(rating_df['rating']) # Nilai minimum rating
max_rating = max(rating_df['rating']) # Nilai maksimal rating

print('total user: {}'.format(num_users))
print('total rating: {}'.format(num_anime))
print('MIN rating {}'.format(min_rating))
print('MAX rating: {}'.format(max_rating))

"""Berdasarkan output diatas, dapat dilihat bahwa pada rating_df terdapat:

  * total user: 901
  * total rating: 4461
  * MIN rating: 1
  * MAX rating: 10

## Train Test Split

Train Test Split adalah metode yang digunakan untuk membagi dataset menjadi dua bagian: satu untuk melatih model (training set) dan satu lagi untuk menguji model (testing set). Biasanya, data dibagi dengan proporsi tertentu, misalnya 90% untuk training dan 10% untuk testing.

Alasan: Proses ini dilakukan agar dapat mengevaluasi kinerja model secara objektif. Dengan memisahkan data uji, kita dapat mengukur seberapa baik model memprediksi data baru yang tidak pernah dilihat sebelumnya, yang merupakan indikator penting dari kemampuan generalisasi model.
"""

rating_df = rating_df.sample(frac=1, random_state=18)

rating_df

"""Berdasarkan output diatas, proses shuffling atau pengacakan berhasil dilakukan"""

x_df = rating_df[['user', 'anime']].values # Membuat variabel x_df untuk mencocokkan data user dan anime menjadi satu value
y_df = rating_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values # Membuat variabel y_df untuk

"""Pemisahan rating_df menjadi dua bagian ke x_df dan y_df untuk proses Train Test Split berhasil dilakukan."""

# Membagi menjadi 90% data train dan 10% data validasi
train_indices = int(0.9 * rating_df.shape[0])
x_train, x_val, y_train, y_val = (
    x_df[:train_indices],
    x_df[train_indices:],
    y_df[:train_indices],
    y_df[train_indices:]
)

"""Proses Train Test Split telah dilakukan ke empat variabel berbebeda dengan komposisi 0.9 untuk train dan 0.1 untuk val. Berikut adalah keempatnya:

  * x_train
  * x_val
  *  y_train
  *  y_val

Berdasarkan keempat output diatas, terbukti bahwa proses Train Test Split telah berhasil dilakukan dan berhasil ditampung pada keempat variabel yang telah dibuat.
"""

print("panjang array dari x_train : " + str(len(x_train)))
x_train

print("panjang array dari x_val : " + str(len(x_val)))
x_val

print("panjang array dari y_train : " + str(len(y_train)))
y_train

print("panjang array dari y_val : " + str(len(y_val)))
y_val

"""Berdasarkan keempat output diatas, terbukti bahwa proses Train Test Split telah berhasil dilakukan dan berhasil ditampung pada keempat variabel yang telah dibuat.

# Modelling and Result

## Content-Based Filtering

Content-Based Filtering adalah metode yang digunakan dalam sistem rekomendasi untuk memberikan saran kepada pengguna berdasarkan item-item yang telah mereka sukai atau pilih sebelumnya. Metode ini berfokus pada karakteristik atau konten dari item yang ingin direkomendasikan.

Kelebihan Content-Based Filtering:

  * Personalisasi: Dapat memberikan rekomendasi yang sangat personal karena didasarkan pada preferensi sebelumnya dari pengguna itu sendiri.
  * Transparansi: Mudah untuk menjelaskan mengapa suatu item direkomendasikan, karena rekomendasi didasarkan pada fitur-fitur item yang telah disukai pengguna.

Kekurangan Content-Based Filtering:

   * Keterbatasan Diversifikasi: Cenderung merekomendasikan item yang mirip dengan yang sudah diketahui pengguna, sehingga kurang memberikan kejutan atau item baru yang berbeda.
   * Ketergantungan pada Konten: Memerlukan data yang cukup tentang konten item untuk bekerja dengan baik, dan kualitas rekomendasi sangat bergantung pada kualitas deskripsi item tersebut.

Pendekatan ini menggunakan atribut-atribut atau fitur-fitur item untuk menentukan kesamaan antara item yang ada. Dalam konteks proyek ini, content-based filtering akan memberikan rekomendasi buku berdasarkan genre dari anime yang ada. Model akan memberikan rekomendasi buku yang memiliki author yang sama.

### Modelling
"""

# Inisialisasi TfidfVectorizer
tf_id = TfidfVectorizer()
tf_id.fit(anime_df['genre'])
tf_id.get_feature_names_out()

"""Output diatas adalah array yang berisi nilai-nilai yang ada pada kolom genre"""

tfidf_matrix = tf_id.fit_transform(anime_df['genre'])
tfidf_matrix.shape # Melihat ukuran matrix tfidf

"""Berdasarkan output diatas, dapat dilihat bahwa ukuran matriksnya sebesar 12017 x 47"""

tfidf_matrix.todense()

"""Berdasarkan output diatas, proses operasi menggunakan todense() sudah berhasil dilakukan"""

## Membuat dataframe untuk melihat tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf_id.get_feature_names_out(),
    index=anime_df.name
).sample(17, axis=1).sample(7, axis=0)

"""Berdasarkan output diatas, dataframe berhasil dibuat dengan data dari matriks yang sudah dibuat sebelumnya"""

# Proses perhitungan cosine_similarity
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Berdasarkan output diatas, proses perhitungan cosine_similarity telah berhasil dilakukan."""

# Membuat dataframe dari variabel cosine_sim
cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_df['name'], columns=anime_df['name'])
print('Ukuran Dataframe : ', cosine_sim_df.shape)

"""Berdasarkan output diatas, proses pembuatan dataframe berhasil dilakukan dan dataframe memiliki ukuran 12017 x 12017."""

# Melihat similarity matrix pada data
cosine_sim_df.sample(5, axis=1).sample(7, axis=0)

"""Output diatas adalah tampilan dari dataframe yang telah dibuat."""

def anime_recommendations(name, similarity_data=cosine_sim_df, items=anime_df[['name', 'genre']], k=5):
    index = similarity_data.loc[:,name].to_numpy().argpartition(range(-1, -k, -1))
    closest_data = similarity_data.columns[index[-1:-(k+2):-1]]
    closest_data = closest_data.drop(name, errors='ignore')

    return pd.DataFrame(closest_data).merge(items).head(k)

"""Function utama yang digunakan untuk pembuatan model Content Based telah berhasil dibuat

### Result
"""

anime_df[anime_df.name.eq('Naruto')]

"""Untuk contoh atau simulasi penggunaan model, kita gunakan naruto yang ber-genre Action, Comedy, Martial Arts, Shounen, Super P..."""

recommendations_result = anime_recommendations('Naruto')
recommendations_result

"""Berikut ini adalah hasil dari Top-N Recommendation menggunakan Content-Based Filterting. Proses penggunaan model berhasil dilakukan dan model dapat memberikan hasil rekomendasi berdasarkan input yang diberikan.

Pada contoh diatas, model berhasil memberikan rekomendasi anime yang juga ber-genre Action, comedy, martial berdasarkan input yang diberikan, yaitu Naruto yang juga bergenre Action, comedy, martial

**Model telah dapat berfungsi dengan baik.**

## Collaborative Filtering

Collaborative Filtering adalah teknik yang digunakan dalam sistem rekomendasi untuk memberikan saran kepada pengguna berdasarkan preferensi atau perilaku pengguna lain yang memiliki kesamaan. Teknik ini mengumpulkan dan menganalisis sejumlah besar informasi tentang perilaku pengguna, aktivitas, atau preferensi dan memprediksi apa yang pengguna akan suka berdasarkan kesamaan dengan pengguna lain.

Kelebihan Collaborative Filtering:

  * Diversifikasi Rekomendasi: Dapat memberikan rekomendasi yang beragam karena didasarkan pada preferensi dari banyak pengguna.
  * Tidak Bergantung pada Konten: Tidak memerlukan pengetahuan tentang konten item, sehingga dapat bekerja dengan item yang memiliki sedikit atau tanpa data konten sama sekali.

Kekurangan Collaborative Filtering:

   
  * Masalah Cold Start: Sulit untuk memberikan rekomendasi kepada pengguna baru atau untuk item baru yang belum memiliki data interaksi.
  * Scalability: Dapat menjadi tantangan ketika jumlah pengguna dan item sangat besar karena membutuhkan komputasi yang intensif.
  * Collaborative Filtering bekerja dengan baik ketika ada cukup data dari pengguna, tetapi bisa menjadi kurang efektif jika data tersebut jarang atau tidak lengkap. Oleh karena itu, sering kali digunakan dalam kombinasi dengan teknik lain untuk meningkatkan kinerja sistem rekomendasi.

### Modeling
"""

class RecommenderNet(Model):

  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size

    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)

    self.anime_embedding = layers.Embedding(
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )

    self.anime_bias = layers.Embedding(num_anime, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])

    dot_user_anime = tensorflow.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tensorflow.nn.sigmoid(x)

"""Function utama yang digunakan untuk pembuatan model Collaborative Filtering telah berhasil dibuat"""

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model
model.compile(
    loss = keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[keras.metrics.RootMeanSquaredError()]
)

"""Inisiasi model telah berhasil dilakukan"""

early_stopper = EarlyStopping(monitor='val_root_mean_squared_error',
                              patience=5,
                              verbose=1,
                              restore_best_weights=True)

"""nisiasi Callback Early Stopper yang akan memantau proses training model. Model akan berhenti jika val_root_mean_squared_error tidak mengalami penurunan lagi selama 5 epochs. Setelah berhenti, model pada epoch tertentu yang memiliki performa terbaik akan dipertahankan"""

history = model.fit(
          x = x_train,
          y = y_train,
          batch_size = 8,
          epochs = 100,
          callbacks = [early_stopper],
          validation_data = (x_val, y_val)
)

"""### Result"""

user_id = rating_df.user_id.sample(1).iloc[0]
anime_reviewed_by_user = rating_df[rating_df.user_id == user_id]
anime_not_reviewed = anime_df[~anime_df['anime_id'].isin(anime_reviewed_by_user.anime_id.values)]['anime_id']
anime_not_reviewed = list(
    set(anime_not_reviewed)
    .intersection(set(anime_to_anime.keys()))
)
anime_not_reviewed = [[anime_to_anime.get(x)] for x in anime_not_reviewed]
user_encoder = user_to_user.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_reviewed), anime_not_reviewed)
)

rating = model.predict(user_anime_array).flatten()

top_rating_indices = rating.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encode_to_anime.get(anime_not_reviewed[x][0]) for x in top_rating_indices
]

print('List recommendations anime untuk users : {}'.format(user_id))
print('====' * 9)
print('Anime dengan skor review tinggi dari user ')
print('=====' * 8)

top_anime_user = (
    anime_reviewed_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)

print('====' * 8)
print('Top 10 anime recommendation')
print('====' * 8)

recommended_anime = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)

"""Berikut ini adalah hasil dari Top-N Recommendation menggunakan Collaborative Filterting. Proses penggunaan model berhasil dilakukan dan model dapat memberikan hasil rekomendasi berdasarkan rating dari user tertentu dan memberikan rekomendasi anime lainnya yang cocok untuk user tersebut.

Pada contoh diatas, model berhasil memberikan rekomendasi film untuk user nomor 18731 yang pernah memberikan skor rating tinggi ke film dan genre:

  * One Punch Man : Action, Comedy, Parody, Sci-Fi, Seinen, Super Power, Supernatural
  * Ookami to Koushinryou II : Adventure, Fantasy, Historical, Romance
  * Dragon Ball Z : Action, Adventure, Comedy, Fantasy, Martial Arts, Shounen, Super Power
  * Darker than Black: Kuro no Keiyakusha : Action, Mystery, Sci-Fi, Super Power
  * Sword Art Online : Action, Adventure, Fantasy, Game, Romance

Model memberikan 10 rekomendasi berupa film dengan genre:

  * Haibane Renmei : Drama, Fantasy, Mystery, Psychological, Slice of Life
  * Gantz 2nd Stage : Action, Drama, Horror, Psychological, Sci-Fi, Supernatural
  * Gantz : Action, Drama, Horror, Psychological, Sci-Fi, Supernatural
  * Catand#039;s Eye : Action, Adventure, Comedy, Mystery, Romance
  * Nissan Serena x One Piece 3D: Mugiwara Chase - Sennyuu!! Sauzando Sanii-gou : Comedy, Fantasy, Shounen
  * Kuro no Sumika: Chronus : Psychological
  * Mayoi Neko Overrun! Specials : Comedy, Ecchi
  * Gilgamesh : Drama, Fantasy, Sci-Fi, Supernatural
  * Rhea Gall Force : Action, Mecha, Military, Sci-Fi
  * Shoujo Sect : Comedy, Hentai, Romance, Yuri

Model telah dapat berfungsi dengan cukup baik.

# Evaluation



Untuk mengukur bagaimana performa dari model yang telah dibuat, diperlukannya metriks evaluasi untuk mengevaluasi model sistem rekomendasi film. Berikut adalah rincian metrik yang digunakan untuk tiap pendekatan:

  * Content-Based Filtering : Precision
  * Collaborative Filtering : Root Mean Squared Error

Berikut ini adalah penjelasan mengenai setiap metrik beserta hasil perhitungan metrik dari model yang telah dibuat :

   * Content-Based Filtering : Precision

       * Precision

        Presisi merupakan ukuran yang menilai efektivitas model klasifikasi dalam mengidentifikasi label positif. Ukuran ini merupakan perbandingan antara jumlah prediksi yang benar-benar positif dengan keseluruhan hasil yang diprediksi sebagai positif, termasuk yang sebenarnya negatif.

        Berikut adalah formula dan cara kerja dari Precision :

            * Formula

            Precision = TP/(TP+FP)

            Dalam Konteks sistem rekomendasi menjadi:

            Precision

            * Cara Kerja

            Formula tersebut mengukur presisi dalam konteks sistem rekomendasi. Presisi dihitung dengan membagi jumlah rekomendasi yang relevan dengan jumlah total item yang direkomendasikan. Jadi, jika sebuah sistem merekomendasikan 10 film dan hanya 6 yang relevan atau disukai oleh pengguna, maka presisi sistem tersebut adalah 0.6 atau 60%. Ini menunjukkan seberapa akurat sistem dalam memberikan rekomendasi yang sesuai dengan kebutuhan atau selera pengguna.

   * Colaborative Filtering : Root Mean Squared Error

        * Root Mean Squared Error

        Root Mean Square Error (RMSE) adalah metrik yang sering digunakan dalam machine learning untuk mengukur seberapa baik sebuah model prediktif dapat memperkirakan nilai yang sebenarnya. RMSE merupakan akar kuadrat dari rata-rata perbedaan kuadrat antara nilai yang diprediksi oleh model dan nilai yang sebenarnya (nilai aktual).

        Berikut ini adalah formula dan cara kerja dari Root Mean Squared Error :

           * Formula

            RMSE = sqrt [(Σ(Pi – Oi)²) / n]

            * Cara Kerja

            RMSE menghitung akar kuadrat dari rata-rata perbedaan kuadrat antara nilai yang diprediksi oleh model dan nilai sebenarnya. Proses kerjanya melibatkan beberapa langkah. Pertama, untuk setiap titik data, kita menghitung selisih antara prediksi model dan nilai aktual. Selisih ini kemudian dikuadratkan untuk menghilangkan nilai negatif dan memberikan bobot lebih pada kesalahan yang lebih besar. Setelah itu, kita menghitung rata-rata dari nilai-nilai kuadrat tersebut. Terakhir, kita mengambil akar kuadrat dari rata-rata ini untuk mendapatkan RMSE.

## Content-Based Filtering
"""

# Calculate precision based on title and genre
def calculate_precision(name, genre):
    name_genre_anime = anime_df[(anime_df['name'] ==name) & (anime_df['genre'] == genre)]
    recommended_animes = animes_recommendations(name)
    relevant_animes = recommended_animes[(recommended_animes['genre'] == genre)]
    precision = len(relevant_animes['genre'] == genre) / len(recommended_animes['genre'] == genre)

    return precision

def calculate_precision(name, genre):
    if not name or not genre:
        return 0.0  # Default precision for invalid/missing values

    precision = len(relevant_animes['genre'] == genre) / len(recommended_animes['genre'] == genre)
    return precision

"""Function utama yang digunakan untuk menghitung skor Precision dari model Content-Based Filtering telah berhasil dibuat."""

genre_anime_df = anime_df.groupby('genre').first().reset_index()[['genre', 'name']]
genre_anime_df

"""Dataframe diatas adalah dataframe yang digunakan untuk mengecek skor Presisi untuk setiap rekomendasi dari tiap genre. Dataframe tersebut berisi pasangan name dan genre dari tiap genre."""

unique_genres = anime_df['genre'].unique()
unique_genres

"""Berdasarkan hasil diatas, unique_genre menampung array yang berisi setiap genre yang ada.

## Collaborative Filtering
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""

Berdasarkan plot tersebut, proses training model berhenti pada epoch ke 14 (epochs 1 dimulai dari nomor 0 pada plot) karena callbacks yang berisi early stopper. early stopper menghentikan proses training karena model tidak menunjukkan penurunan yang lebih keci dari val_root_mean_squared_error pada epochs ke-14 selama 5 epochs berturut-turut.

Kemudian, model pada epochs ke 14 yang dipertahankan karena pada epochs tersebut model memiliki performa yang terbaik. Berikut adalah hasil dari metriks pada epocs tersebut:

  * loss: 0.5141
  * root_mean_squared_error: 0.1280
  * val_loss: 0.5211
  * val_root_mean_squared_error: 0.1389
"""